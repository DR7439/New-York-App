{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start='2021-01', end='2024-04', freq='ME')\n",
    "print(\"Date range:\", date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Current Working Directory:\", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_parquets\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"Directory {data_dir} does not exist\")\n",
    "else:\n",
    "    # List all files in the directory to check for existence and naming\n",
    "    all_files_in_dir = os.listdir(data_dir)\n",
    "    print(f\"Files in directory {data_dir}: {all_files_in_dir}\")\n",
    "\n",
    "all_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_range:\n",
    "    search_pattern = os.path.join(data_dir, f\"yellow_{date.strftime('%Y_%m')}*.parquet\")\n",
    "    print(f\"Searching for files with pattern: {search_pattern}\")\n",
    "    files = glob.glob(search_pattern)\n",
    "    if files:\n",
    "        print(f\"Files found for pattern {search_pattern}: {files}\")\n",
    "    all_files.extend(files)  # Add the found files to the list\n",
    "\n",
    "print(\"All files found:\", all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of files found:\", len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"File path for yellow_2021_01:\", r\"/data-analytics/Datasets/taxi_parquets/yellow_2021_01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renaming_yellow_to_standard(df):\n",
    "    \"\"\" \n",
    "    Function for renaming the columns of a dataset to standard names, which will ease the cleaning process\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df.rename(columns={\n",
    "            'tpep_pickup_datetime': 'pickup_datetime', \n",
    "            'tpep_dropoff_datetime': 'dropoff_datetime', \n",
    "            'PULocationID': 'pickup_zone', \n",
    "            'DOLocationID': 'dropoff_zone'\n",
    "        }, inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Warning: The input is not a DataFrame in renaming_yellow_to_standard\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_float_to_int(df):\n",
    "    \"\"\" \n",
    "    Function for converting datatypes of specific columns of a DataFrame to appropriate types.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        print(\"Converting float columns to int...\")\n",
    "        if \"RatecodeID\" in df.columns:\n",
    "            df[\"RatecodeID\"] = df[\"RatecodeID\"].fillna(0).astype(\"int32\")\n",
    "        if \"passenger_count\" in df.columns:\n",
    "            df[\"passenger_count\"] = df[\"passenger_count\"].fillna(0).astype(\"int32\")\n",
    "        if \"pickup_zone\" in df.columns:\n",
    "            df[\"pickup_zone\"] = df[\"pickup_zone\"].fillna(0).astype(\"int32\")\n",
    "        if \"dropoff_zone\" in df.columns:\n",
    "            df[\"dropoff_zone\"] = df[\"dropoff_zone\"].fillna(0).astype(\"int32\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Warning: The input is not a DataFrame in convert_float_to_int\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Working Directory:\", cwd)\n",
    "\n",
    "taxi_zone_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_other\")\n",
    "\n",
    "\n",
    "# Define the directory where the data is located relative to the current working directory\n",
    "print(\"Taxi Zone CSV Directory:\", taxi_zone_dir)\n",
    "\n",
    "# Define the file path relative to the data directory\n",
    "taxi_zone_path = os.path.join(cwd, taxi_zone_dir, \"taxi_zone_lookup.csv\")\n",
    "\n",
    "taxi_zone = pd.read_csv(taxi_zone_path, keep_default_na=True, delimiter=\",\", skipinitialspace=True, encoding=\"Windows-1252\")\n",
    "\n",
    "def valid_zones_1(df):\n",
    "    manhattan_df = df[df[\"Borough\"] == \"Manhattan\"]\n",
    "    unique_zones = manhattan_df[\"LocationID\"].unique()\n",
    "    \n",
    "    print(f\"Number of Unique Zones: {len(unique_zones)}\")\n",
    "    print(\"List of Unique Zones:\", unique_zones)\n",
    "\n",
    "valid_zones_1(taxi_zone)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get unique zones for Manhattan\n",
    "def get_manhattan_zones(df):\n",
    "    manhattan_df = df[df[\"Borough\"] == \"Manhattan\"]\n",
    "    unique_zones = manhattan_df[\"LocationID\"].unique()\n",
    "    return set(unique_zones)\n",
    "\n",
    "# Get the unique Manhattan zones from the taxi_zone DataFrame\n",
    "manhattan_zones = get_manhattan_zones(taxi_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to check pickup and dropoff zones\n",
    "def check_zones(df, manhattan_zones):\n",
    "    # Check if both pickup_zone and dropoff_zone are not in manhattan_zones\n",
    "    invalid_zones = df[~df[\"pickup_zone\"].isin(manhattan_zones) & ~df[\"dropoff_zone\"].isin(manhattan_zones)]\n",
    "    \n",
    "    print(f\"Invalid zones count: {invalid_zones.shape[0]}\")\n",
    "    \n",
    "    if not invalid_zones.empty:\n",
    "        print(\"Examples of rows with invalid zones:\")\n",
    "        print(invalid_zones.head())  # Print first few invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_yellow_invalid_rows(df, manhattan_zones):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Drop duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        if \"airport_fee\" in df.columns:\n",
    "            df = df.drop(\"airport_fee\", axis=1)\n",
    "        if \"Airport_fee\" in df.columns:\n",
    "            df = df.drop(\"Airport_fee\", axis=1)\n",
    "            \n",
    "        # Drop rows where passenger_count == 0 or >= 6\n",
    "        df = df[(df[\"passenger_count\"] > 0) & (df[\"passenger_count\"] < 6)]\n",
    "        \n",
    "        # Drop rows where fare_amount or total_amount <= 0\n",
    "        df = df[(df[\"fare_amount\"] > 0) & (df[\"total_amount\"] > 0)]\n",
    "        \n",
    "        # Drop rows where extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, or congestion_surcharge < 0\n",
    "        df = df[(df[\"extra\"] >= 0) & (df[\"mta_tax\"] >= 0) & (df[\"tip_amount\"] >= 0) & \n",
    "                (df[\"tolls_amount\"] >= 0) & (df[\"improvement_surcharge\"] >= 0) & \n",
    "                (df[\"congestion_surcharge\"] >= 0)]\n",
    "        \n",
    "        # Drop rows where pickup_datetime == dropoff_datetime\n",
    "        df = df[df[\"pickup_datetime\"] != df[\"dropoff_datetime\"]]\n",
    "        \n",
    "        # Drop rows where trip_distance <= 0\n",
    "        df = df[df[\"trip_distance\"] > 0]\n",
    "        \n",
    "        # Drop rows where RateCodeID != 1-6\n",
    "        df = df[df[\"RatecodeID\"].isin([1, 2, 3, 4, 5, 6])]\n",
    "        \n",
    "        # Drop rows where payment_type == 4\n",
    "        df = df[df[\"payment_type\"] != 4]\n",
    "        \n",
    "        # Drop rows where both pickup_zone and dropoff_zone are not Manhattan zones\n",
    "        df = df[df[\"pickup_zone\"].isin(manhattan_zones) | df[\"dropoff_zone\"].isin(manhattan_zones)]\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: Input is not a DataFrame\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_yellow_columns(df):\n",
    "    columns_to_drop = [\"VendorID\", \"trip_distance\", \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\", \n",
    "                       \"fare_amount\", \"extra\", \"mta_tax\", \"improvement_surcharge\", \"tip_amount\", \n",
    "                       \"tolls_amount\", \"total_amount\", \"congestion_surcharge\"]\n",
    "    \n",
    "    # Drop only the columns that exist in the DataFrame\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(df):\n",
    "    \"\"\"\n",
    "    Drops any rows with missing values from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to clean.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "        int: The number of rows that were dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        initial_row_count = df.shape[0]\n",
    "        df = df.dropna()\n",
    "        final_row_count = df.shape[0]\n",
    "        rows_dropped = initial_row_count - final_row_count\n",
    "        print(f\"Number of rows dropped: {rows_dropped}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_yellow_parquet_files(file_paths, manhattan_zones):\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        print(\"DF Shape OLD\", df.shape)\n",
    "\n",
    "        # Apply the cleaning functions\n",
    "        df = renaming_yellow_to_standard(df)\n",
    "        df = convert_float_to_int(df)\n",
    "        df = drop_yellow_invalid_rows(df, manhattan_zones)\n",
    "        df = drop_yellow_columns(df)\n",
    "        df = drop_missing_values(df)\n",
    "\n",
    "        print(\"DF Shape NEW\", df.shape)\n",
    "\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_dfs.append(df)\n",
    "        \n",
    "        # Save the cleaned DataFrame back to a parquet file (optional)\n",
    "        cleaned_file_path = file_path.replace('.parquet', '_cleaned.parquet')\n",
    "        df.to_parquet(cleaned_file_path)\n",
    "        print(f\"Saved cleaned file: {cleaned_file_path}\")\n",
    "    \n",
    "    # Concatenate all cleaned DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_files(data_dir, date_range):\n",
    "    all_files = []\n",
    "    \n",
    "    for date in date_range:\n",
    "        search_pattern = os.path.join(data_dir, f\"yellow_{date.strftime('%Y_%m')}*.parquet\")\n",
    "        files = glob.glob(search_pattern)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "# Usage example:\n",
    "data_dir = \"c:\\\\Users\\\\35385\\\\Desktop\\\\CS_Summer_2024\\\\Shared_GH\\\\New-York-App\\\\data-analytics\\\\cleaning\\\\..\\\\Datasets\\\\taxi_parquets\\\\\"\n",
    "date_range = pd.date_range(start=\"2021-01-01\", end=\"2024-03-31\", freq=\"MS\")\n",
    "\n",
    "file_paths = get_parquet_files(data_dir, date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = clean_yellow_parquet_files(file_paths, manhattan_zones)\n",
    "\n",
    "# Save the final concatenated DataFrame to a parquet file (optional)\n",
    "final_df.to_parquet('c:\\\\Users\\\\35385\\\\Desktop\\\\CS_Summer_2024\\\\Shared_GH\\\\New-York-App\\\\data-analytics\\\\cleaning\\\\..\\\\Datasets\\\\taxi_parquets\\\\yellow_final_cleaned.parquet')\n",
    "print(\"Saved final concatenated DataFrame: yellow_final_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Current Working Directory:\", cwd)\n",
    "\n",
    "# Define the directory where the data is located relative to the current working directory\n",
    "print(\"Data Directory:\", data_dir)\n",
    "\n",
    "# Define the file paths relative to the data directory\n",
    "yellow_final_cleaned_path = os.path.join(data_dir, \"yellow_final_cleaned.parquet\")\n",
    "\n",
    "# Print the constructed file paths to verify\n",
    "print(\"yellow_final_cleaned:\", yellow_final_cleaned_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet files using the relative file paths\n",
    "yellow_final_cleaned = pd.read_parquet(yellow_final_cleaned_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_final_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_final_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_final_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow_final_cleaned = calculate_zone_busy_in_chunks(yellow_final_cleaned)\n",
    "# print(yellow_final_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_other\")\n",
    "\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = os.path.join(data_dir, \"yellow_final_cleaned.csv\")\n",
    "\n",
    "# # Save the DataFrame to CSV\n",
    "# yellow_final_cleaned.to_csv(file_path, index=False)\n",
    "\n",
    "# print(\"DataFrame saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start='2021-01', end='2024-04', freq='ME')\n",
    "print(\"Date range:\", date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_parquets\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"Directory {data_dir} does not exist\")\n",
    "else:\n",
    "    # List all files in the directory to check for existence and naming\n",
    "    all_files_in_dir = os.listdir(data_dir)\n",
    "    print(f\"Files in directory {data_dir}: {all_files_in_dir}\")\n",
    "\n",
    "all_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_range:\n",
    "    search_pattern = os.path.join(data_dir, f\"green_{date.strftime('%Y_%m')}*.parquet\")\n",
    "    print(f\"Searching for files with pattern: {search_pattern}\")\n",
    "    files = glob.glob(search_pattern)\n",
    "    if files:\n",
    "        print(f\"Files found for pattern {search_pattern}: {files}\")\n",
    "    all_files.extend(files)  # Add the found files to the list\n",
    "\n",
    "print(\"All files found:\", all_files)\n",
    "print(\"Number of files found:\", len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renaming_green_to_standard(df):\n",
    "    \"\"\" \n",
    "    Functions for renaming the columns of a dataset or list of datasets to standard names, which will ease the cleaning process\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df.rename(columns={\n",
    "            'lpep_pickup_datetime': 'pickup_datetime', \n",
    "            'lpep_dropoff_datetime': 'dropoff_datetime', \n",
    "            'PULocationID': 'pickup_zone', \n",
    "            'DOLocationID': 'dropoff_zone'\n",
    "            }, inplace=True)\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: The list contains non-DataFrame elements renaming_green_to_standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_green_invalid_rows(df, manhattan_zones):\n",
    "\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # Drop duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "            \n",
    "        # Drop \"ehail_fee\" column due to missing values\n",
    "        if \"ehail_fee\" in df.columns:\n",
    "            df = df.drop(columns=[\"ehail_fee\"])\n",
    "            \n",
    "        # Drop rows where passenger_count == 0 or >= 6\n",
    "        df = df[(df[\"passenger_count\"] > 0) & (df[\"passenger_count\"] < 6)]\n",
    "            \n",
    "        # Drop rows where fare_amount or total_amount <= 0\n",
    "        df = df[(df[\"fare_amount\"] > 0) & (df[\"total_amount\"] > 0)]\n",
    "            \n",
    "        # Drop rows where extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, or congestion_surcharge < 0\n",
    "        df = df[(df[\"extra\"] >= 0) & (df[\"mta_tax\"] >= 0) & (df[\"tip_amount\"] >= 0) & \n",
    "                (df[\"tolls_amount\"] >= 0) & (df[\"improvement_surcharge\"] >= 0) & \n",
    "                (df[\"congestion_surcharge\"] >= 0)]\n",
    "            \n",
    "        # Drop rows where pickup_datetime == dropoff_datetime\n",
    "        df = df[df[\"pickup_datetime\"] != df[\"dropoff_datetime\"]]\n",
    "            \n",
    "        # Drop rows where trip_distance <= 0\n",
    "        df = df[df[\"trip_distance\"] > 0]\n",
    "            \n",
    "        # Drop rows where RateCodeID != 1-6\n",
    "        df = df[df[\"RatecodeID\"].isin([1, 2, 3, 4, 5, 6])]\n",
    "            \n",
    "        # Drop rows where payment_type == 4\n",
    "        df = df[df[\"payment_type\"] != 4]\n",
    "            \n",
    "        # Drop rows where both pickup_zone and dropoff_zone are not Manhattan zones\n",
    "        df = df[df[\"pickup_zone\"].isin(manhattan_zones) | df[\"dropoff_zone\"].isin(manhattan_zones)]\n",
    "            \n",
    "    else:\n",
    "        print(\"Warning: Input is not a DataFrame drop_green_invalid_rows\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_green_columns(df):\n",
    "    columns_to_drop = [\"VendorID\", \"trip_distance\", \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\", \n",
    "                       \"fare_amount\", \"extra\", \"mta_tax\", \"improvement_surcharge\", \"tip_amount\", \n",
    "                       \"tolls_amount\", \"total_amount\", \"congestion_surcharge\", \"trip_type\"]\n",
    "    \n",
    "    # Drop only the columns that exist in the DataFrame\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_green_parquet_files(file_paths, manhattan_zones):\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        print(\"DF Shape OLD\", df.shape)\n",
    "\n",
    "        # Apply the cleaning functions\n",
    "        df = renaming_green_to_standard(df)\n",
    "        df = convert_float_to_int(df)\n",
    "        df = drop_green_invalid_rows(df, manhattan_zones)\n",
    "        df = drop_green_columns(df)\n",
    "        df = drop_missing_values(df)\n",
    "\n",
    "        print(\"DF Shape NEW\", df.shape)\n",
    "        \n",
    "        # Append the cleaned DataFrame to the list\n",
    "        cleaned_dfs.append(df)\n",
    "        \n",
    "        # Save the cleaned DataFrame back to a parquet file (optional)\n",
    "        cleaned_file_path = file_path.replace('.parquet', '_cleaned.parquet')\n",
    "        df.to_parquet(cleaned_file_path)\n",
    "        print(f\"Saved cleaned file: {cleaned_file_path}\")\n",
    "    \n",
    "    # Concatenate all cleaned DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_files(data_dir, date_range):\n",
    "    all_files = []\n",
    "    \n",
    "    for date in date_range:\n",
    "        search_pattern = os.path.join(data_dir, f\"green_{date.strftime('%Y_%m')}*.parquet\")\n",
    "        files = glob.glob(search_pattern)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "data_dir = \"c:\\\\Users\\\\35385\\\\Desktop\\\\CS_Summer_2024\\\\Shared_GH\\\\New-York-App\\\\data-analytics\\\\cleaning\\\\..\\\\Datasets\\\\taxi_parquets\\\\\"\n",
    "date_range = pd.date_range(start=\"2021-01-01\", end=\"2024-03-31\", freq=\"MS\")\n",
    "\n",
    "file_paths = get_parquet_files(data_dir, date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_green_df = clean_green_parquet_files(file_paths, manhattan_zones)\n",
    "\n",
    "# Save the final concatenated DataFrame to a parquet file (optional)\n",
    "final_green_df.to_parquet('c:\\\\Users\\\\35385\\\\Desktop\\\\CS_Summer_2024\\\\Shared_GH\\\\New-York-App\\\\data-analytics\\\\cleaning\\\\..\\\\Datasets\\\\taxi_parquets\\\\green_final_cleaned.parquet')\n",
    "print(\"Saved final concatenated DataFrame: green_final_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Begin by loading 1 parquet file as pandas dataframe from each of the 4 TLC genres.\n",
    "Error catching across OSes implemented: cwd, data directory, paths etc.\n",
    "\"\"\"\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"Current Working Directory:\", cwd)\n",
    "\n",
    "# Define the directory where the data is located relative to the current working directory\n",
    "print(\"Data Directory:\", data_dir)\n",
    "\n",
    "# Define the file paths relative to the data directory\n",
    "green_final_cleaned_path = os.path.join(data_dir, \"green_final_cleaned.parquet\")\n",
    "\n",
    "# Print the constructed file paths to verify\n",
    "print(\"green_final_cleaned:\", green_final_cleaned_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet files using the relative file paths\n",
    "green_final_cleaned = pd.read_parquet(green_final_cleaned_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_final_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_final_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_final_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green_final_cleaned = calculate_zone_busy_in_chunks(green_final_cleaned)\n",
    "# print(green_final_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_other\")\n",
    "\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = os.path.join(data_dir, \"green_final_cleaned.csv\")\n",
    "\n",
    "# # Save the DataFrame to CSV\n",
    "# green_final_cleaned.to_csv(file_path, index=False)\n",
    "\n",
    "# print(\"DataFrame saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\35385\\Desktop\\CS_Summer_2024\\Shared_GH\\New-York-App\\data-analytics\\cleaning\\..\\Datasets\\taxi_parquets\n",
      "Files in directory c:\\Users\\35385\\Desktop\\CS_Summer_2024\\Shared_GH\\New-York-App\\data-analytics\\cleaning\\..\\Datasets\\taxi_parquets: ['fhvhv_2021_01.parquet', 'fhvhv_2021_02.parquet', 'fhvhv_2021_03.parquet', 'fhvhv_2021_04.parquet', 'fhvhv_2021_05.parquet', 'fhvhv_2021_06.parquet', 'fhvhv_2021_07.parquet', 'fhvhv_2021_08.parquet', 'fhvhv_2021_09.parquet', 'fhvhv_2021_10.parquet', 'fhvhv_2021_11.parquet', 'fhvhv_2021_12.parquet', 'fhvhv_2022_01.parquet', 'fhvhv_2022_02.parquet', 'fhvhv_2022_03.parquet', 'fhvhv_2022_04.parquet', 'fhvhv_2022_05.parquet', 'fhvhv_2022_06.parquet', 'fhvhv_2022_07.parquet', 'fhvhv_2022_08.parquet', 'fhvhv_2022_09.parquet', 'fhvhv_2022_10.parquet', 'fhvhv_2022_11.parquet', 'fhvhv_2022_12.parquet', 'fhvhv_2023_01.parquet', 'fhvhv_2023_02.parquet', 'fhvhv_2023_03.parquet', 'fhvhv_2023_04.parquet', 'fhvhv_2023_05.parquet', 'fhvhv_2023_06.parquet', 'fhvhv_2023_07.parquet', 'fhvhv_2023_08.parquet', 'fhvhv_2023_09.parquet', 'fhvhv_2023_10.parquet', 'fhvhv_2023_11.parquet', 'fhvhv_2023_12.parquet', 'fhvhv_2024_01.parquet', 'fhvhv_2024_02.parquet', 'fhvhv_2024_03.parquet', 'fhv_2021_01.parquet', 'fhv_2021_02.parquet', 'fhv_2021_03.parquet', 'fhv_2021_04.parquet', 'fhv_2021_05.parquet', 'fhv_2021_06.parquet', 'fhv_2021_07.parquet', 'fhv_2021_08.parquet', 'fhv_2021_09.parquet', 'fhv_2021_10.parquet', 'fhv_2021_11.parquet', 'fhv_2021_12.parquet', 'fhv_2022_01.parquet', 'fhv_2022_02.parquet', 'fhv_2022_03.parquet', 'fhv_2022_04.parquet', 'fhv_2022_05.parquet', 'fhv_2022_06.parquet', 'fhv_2022_07.parquet', 'fhv_2022_08.parquet', 'fhv_2022_09.parquet', 'fhv_2022_10.parquet', 'fhv_2022_11.parquet', 'fhv_2022_12.parquet', 'fhv_2023_01.parquet', 'fhv_2023_02.parquet', 'fhv_2023_03.parquet', 'fhv_2023_04.parquet', 'fhv_2023_05.parquet', 'fhv_2023_06.parquet', 'fhv_2023_07.parquet', 'fhv_2023_08.parquet', 'fhv_2023_09.parquet', 'fhv_2023_10.parquet', 'fhv_2023_11.parquet', 'fhv_2023_12.parquet', 'fhv_2024_01.parquet', 'fhv_2024_02.parquet', 'fhv_2024_03.parquet', 'green_2021_01.parquet', 'green_2021_01_cleaned.parquet', 'green_2021_02.parquet', 'green_2021_02_cleaned.parquet', 'green_2021_03.parquet', 'green_2021_03_cleaned.parquet', 'green_2021_04.parquet', 'green_2021_04_cleaned.parquet', 'green_2021_05.parquet', 'green_2021_05_cleaned.parquet', 'green_2021_06.parquet', 'green_2021_06_cleaned.parquet', 'green_2021_07.parquet', 'green_2021_07_cleaned.parquet', 'green_2021_08.parquet', 'green_2021_08_cleaned.parquet', 'green_2021_09.parquet', 'green_2021_09_cleaned.parquet', 'green_2021_10.parquet', 'green_2021_10_cleaned.parquet', 'green_2021_11.parquet', 'green_2021_11_cleaned.parquet', 'green_2021_12.parquet', 'green_2021_12_cleaned.parquet', 'green_2022_01.parquet', 'green_2022_01_cleaned.parquet', 'green_2022_02.parquet', 'green_2022_02_cleaned.parquet', 'green_2022_03.parquet', 'green_2022_03_cleaned.parquet', 'green_2022_04.parquet', 'green_2022_04_cleaned.parquet', 'green_2022_05.parquet', 'green_2022_05_cleaned.parquet', 'green_2022_06.parquet', 'green_2022_06_cleaned.parquet', 'green_2022_07.parquet', 'green_2022_07_cleaned.parquet', 'green_2022_08.parquet', 'green_2022_08_cleaned.parquet', 'green_2022_09.parquet', 'green_2022_09_cleaned.parquet', 'green_2022_10.parquet', 'green_2022_10_cleaned.parquet', 'green_2022_11.parquet', 'green_2022_11_cleaned.parquet', 'green_2022_12.parquet', 'green_2022_12_cleaned.parquet', 'green_2023_01.parquet', 'green_2023_01_cleaned.parquet', 'green_2023_02.parquet', 'green_2023_02_cleaned.parquet', 'green_2023_03.parquet', 'green_2023_03_cleaned.parquet', 'green_2023_04.parquet', 'green_2023_04_cleaned.parquet', 'green_2023_05.parquet', 'green_2023_05_cleaned.parquet', 'green_2023_06.parquet', 'green_2023_06_cleaned.parquet', 'green_2023_07.parquet', 'green_2023_07_cleaned.parquet', 'green_2023_08.parquet', 'green_2023_08_cleaned.parquet', 'green_2023_09.parquet', 'green_2023_09_cleaned.parquet', 'green_2023_10.parquet', 'green_2023_10_cleaned.parquet', 'green_2023_11.parquet', 'green_2023_11_cleaned.parquet', 'green_2023_12.parquet', 'green_2023_12_cleaned.parquet', 'green_2024_01.parquet', 'green_2024_01_cleaned.parquet', 'green_2024_02.parquet', 'green_2024_02_cleaned.parquet', 'green_2024_03.parquet', 'green_2024_03_cleaned.parquet', 'green_final_cleaned.parquet', 'yellow_2021_01.parquet', 'yellow_2021_01_cleaned.parquet', 'yellow_2021_02.parquet', 'yellow_2021_02_cleaned.parquet', 'yellow_2021_03.parquet', 'yellow_2021_03_cleaned.parquet', 'yellow_2021_04.parquet', 'yellow_2021_04_cleaned.parquet', 'yellow_2021_05.parquet', 'yellow_2021_05_cleaned.parquet', 'yellow_2021_06.parquet', 'yellow_2021_06_cleaned.parquet', 'yellow_2021_07.parquet', 'yellow_2021_07_cleaned.parquet', 'yellow_2021_08.parquet', 'yellow_2021_08_cleaned.parquet', 'yellow_2021_09.parquet', 'yellow_2021_09_cleaned.parquet', 'yellow_2021_10.parquet', 'yellow_2021_10_cleaned.parquet', 'yellow_2021_11.parquet', 'yellow_2021_11_cleaned.parquet', 'yellow_2021_12.parquet', 'yellow_2021_12_cleaned.parquet', 'yellow_2022_01.parquet', 'yellow_2022_01_cleaned.parquet', 'yellow_2022_02.parquet', 'yellow_2022_02_cleaned.parquet', 'yellow_2022_03.parquet', 'yellow_2022_03_cleaned.parquet', 'yellow_2022_04.parquet', 'yellow_2022_04_cleaned.parquet', 'yellow_2022_05.parquet', 'yellow_2022_05_cleaned.parquet', 'yellow_2022_06.parquet', 'yellow_2022_06_cleaned.parquet', 'yellow_2022_07.parquet', 'yellow_2022_07_cleaned.parquet', 'yellow_2022_08.parquet', 'yellow_2022_08_cleaned.parquet', 'yellow_2022_09.parquet', 'yellow_2022_09_cleaned.parquet', 'yellow_2022_10.parquet', 'yellow_2022_10_cleaned.parquet', 'yellow_2022_11.parquet', 'yellow_2022_11_cleaned.parquet', 'yellow_2022_12.parquet', 'yellow_2022_12_cleaned.parquet', 'yellow_2023_01.parquet', 'yellow_2023_01_cleaned.parquet', 'yellow_2023_02.parquet', 'yellow_2023_02_cleaned.parquet', 'yellow_2023_03.parquet', 'yellow_2023_03_cleaned.parquet', 'yellow_2023_04.parquet', 'yellow_2023_04_cleaned.parquet', 'yellow_2023_05.parquet', 'yellow_2023_05_cleaned.parquet', 'yellow_2023_06.parquet', 'yellow_2023_06_cleaned.parquet', 'yellow_2023_07.parquet', 'yellow_2023_07_cleaned.parquet', 'yellow_2023_08.parquet', 'yellow_2023_08_cleaned.parquet', 'yellow_2023_09.parquet', 'yellow_2023_09_cleaned.parquet', 'yellow_2023_10.parquet', 'yellow_2023_10_cleaned.parquet', 'yellow_2023_11.parquet', 'yellow_2023_11_cleaned.parquet', 'yellow_2023_12.parquet', 'yellow_2023_12_cleaned.parquet', 'yellow_2024_01.parquet', 'yellow_2024_01_cleaned.parquet', 'yellow_2024_02.parquet', 'yellow_2024_02_cleaned.parquet', 'yellow_2024_03.parquet', 'yellow_2024_03_cleaned.parquet', 'yellow_final_cleaned.parquet']\n",
      "236\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_parquets\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"Directory {data_dir} does not exist\")\n",
    "else:\n",
    "    # List all files in the directory to check for existence and naming\n",
    "    all_files_in_dir = os.listdir(data_dir)\n",
    "    print(f\"Files in directory {data_dir}: {all_files_in_dir}\")\n",
    "    print(len(all_files_in_dir))\n",
    "all_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_final_cleaned_path = os.path.join(data_dir, \"yellow_final_cleaned.parquet\")\n",
    "green_final_cleaned_path = os.path.join(data_dir, \"green_final_cleaned.parquet\")\n",
    "\n",
    "yellow_final_cleaned = pd.read_parquet(yellow_final_cleaned_path, engine='pyarrow')\n",
    "green_final_cleaned = pd.read_parquet(green_final_cleaned_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow records with pickup_datetime before 2021-01-01:\n",
      "              pickup_datetime    dropoff_datetime  passenger_count  \\\n",
      "410       2020-12-31 23:59:06 2021-01-01 00:02:51                2   \n",
      "476       2020-12-31 21:40:20 2020-12-31 22:16:09                1   \n",
      "491       2020-12-31 23:57:17 2021-01-01 00:17:40                1   \n",
      "568       2020-12-31 18:41:27 2021-01-01 17:52:03                1   \n",
      "798       2020-12-31 23:55:40 2021-01-01 00:24:51                1   \n",
      "...                       ...                 ...              ...   \n",
      "94442717  2002-12-31 22:59:39 2002-12-31 23:05:41                1   \n",
      "95496065  2009-01-01 23:30:39 2009-01-02 00:01:39                1   \n",
      "97101103  2008-12-31 22:52:49 2008-12-31 23:04:09                1   \n",
      "97753088  2009-01-01 00:02:13 2009-01-01 00:48:28                1   \n",
      "100988757 2002-12-31 22:17:10 2002-12-31 22:42:24                1   \n",
      "\n",
      "           pickup_zone  dropoff_zone  \n",
      "410                237           236  \n",
      "476                249           213  \n",
      "491                163           166  \n",
      "568                 90           229  \n",
      "798                231            17  \n",
      "...                ...           ...  \n",
      "94442717           170           170  \n",
      "95496065           237           264  \n",
      "97101103           141           211  \n",
      "97753088            79           148  \n",
      "100988757           50           162  \n",
      "\n",
      "[678 rows x 5 columns]\n",
      "\n",
      "Yellow records with dropoff_datetime before 2021-01-01:\n",
      "              pickup_datetime    dropoff_datetime  passenger_count  \\\n",
      "476       2020-12-31 21:40:20 2020-12-31 22:16:09                1   \n",
      "829       2020-12-31 13:44:02 2020-12-31 13:53:43                1   \n",
      "1462      2020-12-31 21:35:59 2020-12-31 21:38:20                1   \n",
      "1463      2020-12-31 21:43:40 2020-12-31 21:52:26                1   \n",
      "1648      2020-12-31 23:31:29 2020-12-31 23:37:51                1   \n",
      "...                       ...                 ...              ...   \n",
      "94442717  2002-12-31 22:59:39 2002-12-31 23:05:41                1   \n",
      "95496065  2009-01-01 23:30:39 2009-01-02 00:01:39                1   \n",
      "97101103  2008-12-31 22:52:49 2008-12-31 23:04:09                1   \n",
      "97753088  2009-01-01 00:02:13 2009-01-01 00:48:28                1   \n",
      "100988757 2002-12-31 22:17:10 2002-12-31 22:42:24                1   \n",
      "\n",
      "           pickup_zone  dropoff_zone  \n",
      "476                249           213  \n",
      "829                170           226  \n",
      "1462                75            74  \n",
      "1463               263            74  \n",
      "1648               137            79  \n",
      "...                ...           ...  \n",
      "94442717           170           170  \n",
      "95496065           237           264  \n",
      "97101103           141           211  \n",
      "97753088            79           148  \n",
      "100988757           50           162  \n",
      "\n",
      "[558 rows x 5 columns]\n",
      "\n",
      "Green records with pickup_datetime before 2021-01-01:\n",
      "            pickup_datetime    dropoff_datetime  pickup_zone  dropoff_zone  \\\n",
      "3       2020-12-31 23:57:51 2021-01-01 00:04:56          168            75   \n",
      "169919  2009-01-01 01:03:17 2009-01-01 01:07:24           75            74   \n",
      "172280  2009-01-01 00:17:19 2009-01-01 06:01:13           75            23   \n",
      "180622  2008-12-31 23:12:53 2008-12-31 23:27:09          166            75   \n",
      "186244  2009-01-01 00:13:47 2009-01-01 00:15:53           41           166   \n",
      "201826  2008-12-31 19:16:53 2008-12-31 19:34:36           74           264   \n",
      "373471  2009-01-01 00:07:17 2009-01-01 00:10:02           41           152   \n",
      "391309  2009-01-01 00:34:01 2009-01-01 17:05:20           25           202   \n",
      "584835  2009-01-01 02:07:04 2009-01-01 02:20:20           75           237   \n",
      "681997  2008-12-31 17:04:15 2008-12-31 17:55:15           74            42   \n",
      "755107  2009-01-01 06:57:41 2009-01-01 07:22:45          260           263   \n",
      "992645  2008-12-31 22:41:41 2008-12-31 23:01:41          166           264   \n",
      "1160345 2009-01-01 00:37:33 2009-01-01 09:35:35           75           264   \n",
      "\n",
      "         passenger_count  \n",
      "3                      1  \n",
      "169919                 1  \n",
      "172280                 1  \n",
      "180622                 1  \n",
      "186244                 1  \n",
      "201826                 1  \n",
      "373471                 1  \n",
      "391309                 1  \n",
      "584835                 1  \n",
      "681997                 1  \n",
      "755107                 1  \n",
      "992645                 1  \n",
      "1160345                5  \n",
      "\n",
      "Green records with dropoff_datetime before 2021-01-01:\n",
      "            pickup_datetime    dropoff_datetime  pickup_zone  dropoff_zone  \\\n",
      "169919  2009-01-01 01:03:17 2009-01-01 01:07:24           75            74   \n",
      "172280  2009-01-01 00:17:19 2009-01-01 06:01:13           75            23   \n",
      "180622  2008-12-31 23:12:53 2008-12-31 23:27:09          166            75   \n",
      "186244  2009-01-01 00:13:47 2009-01-01 00:15:53           41           166   \n",
      "201826  2008-12-31 19:16:53 2008-12-31 19:34:36           74           264   \n",
      "373471  2009-01-01 00:07:17 2009-01-01 00:10:02           41           152   \n",
      "391309  2009-01-01 00:34:01 2009-01-01 17:05:20           25           202   \n",
      "584835  2009-01-01 02:07:04 2009-01-01 02:20:20           75           237   \n",
      "681997  2008-12-31 17:04:15 2008-12-31 17:55:15           74            42   \n",
      "755107  2009-01-01 06:57:41 2009-01-01 07:22:45          260           263   \n",
      "992645  2008-12-31 22:41:41 2008-12-31 23:01:41          166           264   \n",
      "1160345 2009-01-01 00:37:33 2009-01-01 09:35:35           75           264   \n",
      "\n",
      "         passenger_count  \n",
      "169919                 1  \n",
      "172280                 1  \n",
      "180622                 1  \n",
      "186244                 1  \n",
      "201826                 1  \n",
      "373471                 1  \n",
      "391309                 1  \n",
      "584835                 1  \n",
      "681997                 1  \n",
      "755107                 1  \n",
      "992645                 1  \n",
      "1160345                5  \n"
     ]
    }
   ],
   "source": [
    "cutoff_date = pd.Timestamp('2021-01-01')\n",
    "\n",
    "# Check for records before the cutoff date in yellow_final_cleaned\n",
    "yellow_before_cutoff_pickup = yellow_final_cleaned[yellow_final_cleaned['pickup_datetime'] < cutoff_date]\n",
    "yellow_before_cutoff_dropoff = yellow_final_cleaned[yellow_final_cleaned['dropoff_datetime'] < cutoff_date]\n",
    "\n",
    "# Check for records before the cutoff date in green_final_cleaned\n",
    "green_before_cutoff_pickup = green_final_cleaned[green_final_cleaned['pickup_datetime'] < cutoff_date]\n",
    "green_before_cutoff_dropoff = green_final_cleaned[green_final_cleaned['dropoff_datetime'] < cutoff_date]\n",
    "\n",
    "# Print the results\n",
    "print(\"Yellow records with pickup_datetime before 2021-01-01:\")\n",
    "print(yellow_before_cutoff_pickup)\n",
    "\n",
    "print(\"\\nYellow records with dropoff_datetime before 2021-01-01:\")\n",
    "print(yellow_before_cutoff_dropoff)\n",
    "\n",
    "print(\"\\nGreen records with pickup_datetime before 2021-01-01:\")\n",
    "print(green_before_cutoff_pickup)\n",
    "\n",
    "print(\"\\nGreen records with dropoff_datetime before 2021-01-01:\")\n",
    "print(green_before_cutoff_dropoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in green_final_cleaned to match yellow_final_cleaned order\n",
    "green_final_cleaned.rename(columns={\n",
    "    'pickup_datetime': 'pickup_datetime',\n",
    "    'dropoff_datetime': 'dropoff_datetime',\n",
    "    'pickup_zone': 'pickup_zone',\n",
    "    'dropoff_zone': 'dropoff_zone',\n",
    "    'passenger_count': 'passenger_count'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103683400 entries, 0 to 103683399\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   pickup_datetime   datetime64[us]\n",
      " 1   dropoff_datetime  datetime64[us]\n",
      " 2   passenger_count   int32         \n",
      " 3   pickup_zone       int32         \n",
      " 4   dropoff_zone      int32         \n",
      "dtypes: datetime64[us](2), int32(3)\n",
      "memory usage: 2.7 GB\n",
      "None\n",
      "      pickup_datetime    dropoff_datetime  passenger_count  pickup_zone  \\\n",
      "0 2021-01-01 00:30:10 2021-01-01 00:36:12                1          142   \n",
      "1 2021-01-01 00:51:20 2021-01-01 00:52:19                1          238   \n",
      "2 2021-01-01 00:31:49 2021-01-01 00:48:21                1           68   \n",
      "3 2021-01-01 00:16:29 2021-01-01 00:24:30                1          224   \n",
      "4 2021-01-01 00:12:29 2021-01-01 00:30:34                1           90   \n",
      "5 2021-01-01 00:26:12 2021-01-01 00:39:46                2          263   \n",
      "6 2021-01-01 00:15:52 2021-01-01 00:38:07                3          164   \n",
      "7 2021-01-01 00:10:46 2021-01-01 00:32:58                2          138   \n",
      "8 2021-01-01 00:31:06 2021-01-01 00:38:52                5          142   \n",
      "9 2021-01-01 00:42:11 2021-01-01 00:44:24                5           50   \n",
      "\n",
      "   dropoff_zone  \n",
      "0            43  \n",
      "1           151  \n",
      "2            33  \n",
      "3            68  \n",
      "4            40  \n",
      "5           142  \n",
      "6           255  \n",
      "7           166  \n",
      "8            50  \n",
      "9           142  \n"
     ]
    }
   ],
   "source": [
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([yellow_final_cleaned, green_final_cleaned], ignore_index=True)\n",
    "\n",
    "# Check the combined DataFrame\n",
    "print(combined_df.info())\n",
    "print(combined_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_zone_busy_in_chunks(df, chunk_size=10**6):\n",
    "    zone_busy_list = []\n",
    "\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "\n",
    "        # Combine pickup and dropoff data into a single DataFrame\n",
    "        pickup_data = chunk[['pickup_datetime', 'passenger_count', 'pickup_zone']].rename(columns={'pickup_datetime': 'datetime', 'pickup_zone': 'zone'})\n",
    "        dropoff_data = chunk[['dropoff_datetime', 'passenger_count', 'dropoff_zone']].rename(columns={'dropoff_datetime': 'datetime', 'dropoff_zone': 'zone'})\n",
    "        combined_data = pd.concat([pickup_data, dropoff_data])\n",
    "\n",
    "        # Round datetime to the nearest hour\n",
    "        combined_data['datetime'] = combined_data['datetime'].dt.round('h')\n",
    "\n",
    "        # Extract the required time components\n",
    "        combined_data['datetime_formatted'] = combined_data['datetime'].dt.strftime('%Y-%m-%d-%H')\n",
    "        combined_data['hour'] = combined_data['datetime'].dt.hour\n",
    "        combined_data['day_of_week'] = combined_data['datetime'].dt.dayofweek\n",
    "        combined_data['week'] = combined_data['datetime'].dt.isocalendar().week\n",
    "        combined_data['month'] = combined_data['datetime'].dt.month - 1  # Convert to 0-11 for Jan-Dec\n",
    "        combined_data['day_of_month'] = combined_data['datetime'].dt.day\n",
    "        combined_data['year_month'] = combined_data['datetime'].dt.to_period('M').astype(str)\n",
    "\n",
    "        # Group by datetime_formatted, hour, day_of_week, week, month, day_of_month, year_month, and zone, summing passenger counts\n",
    "        zone_busy_chunk = combined_data.groupby(['datetime_formatted', 'hour', 'day_of_week', 'week', 'month', 'day_of_month', 'year_month', 'zone'], as_index=False)['passenger_count'].sum()\n",
    "        zone_busy_list.append(zone_busy_chunk)\n",
    "\n",
    "    # Concatenate all the chunk results\n",
    "    zone_busy_df = pd.concat(zone_busy_list, ignore_index=True)\n",
    "\n",
    "    # Final grouping to combine all the chunks\n",
    "    zone_busy_df = zone_busy_df.groupby(['datetime_formatted', 'hour', 'day_of_week', 'week', 'month', 'day_of_month', 'year_month', 'zone'], as_index=False)['passenger_count'].sum()\n",
    "\n",
    "    return zone_busy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        datetime_formatted  hour  day_of_week  week  month  day_of_month  \\\n",
      "0            2001-01-01-00     0            0     1      0             1   \n",
      "1            2001-01-01-00     0            0     1      0             1   \n",
      "2            2001-01-01-00     0            0     1      0             1   \n",
      "3            2001-01-01-00     0            0     1      0             1   \n",
      "4            2001-01-01-00     0            0     1      0             1   \n",
      "...                    ...   ...          ...   ...    ...           ...   \n",
      "3344022      2028-12-07-05     5            3    49     11             7   \n",
      "3344023      2029-05-05-09     9            5    18      4             5   \n",
      "3344024      2029-05-05-12    12            5    18      4             5   \n",
      "3344025      2098-09-11-02     2            3    37      8            11   \n",
      "3344026      2098-09-11-03     3            3    37      8            11   \n",
      "\n",
      "        year_month  zone  passenger_count  \n",
      "0          2001-01    43                1  \n",
      "1          2001-01    48                1  \n",
      "2          2001-01   132                2  \n",
      "3          2001-01   161                2  \n",
      "4          2001-01   239                2  \n",
      "...            ...   ...              ...  \n",
      "3344022    2028-12   170                3  \n",
      "3344023    2029-05   231                1  \n",
      "3344024    2029-05   249                1  \n",
      "3344025    2098-09   163                5  \n",
      "3344026    2098-09   231                5  \n",
      "\n",
      "[3344027 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#15 minute runtime\n",
    "combined_df = calculate_zone_busy_in_chunks(combined_df)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3344027 entries, 0 to 3344026\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   datetime_formatted  object\n",
      " 1   hour                int32 \n",
      " 2   day_of_week         int32 \n",
      " 3   week                UInt32\n",
      " 4   month               int32 \n",
      " 5   day_of_month        int32 \n",
      " 6   year_month          object\n",
      " 7   zone                int32 \n",
      " 8   passenger_count     int32 \n",
      "dtypes: UInt32(1), int32(6), object(2)\n",
      "memory usage: 143.5+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_formatted</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>year_month</th>\n",
       "      <th>zone</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-01-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-01-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-01-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>161</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-01-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001-01-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001-01-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001-01-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>230</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2001-01-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2001-01-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-01</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datetime_formatted  hour  day_of_week  week  month  day_of_month year_month  \\\n",
       "0      2001-01-01-00     0            0     1      0             1    2001-01   \n",
       "1      2001-01-01-00     0            0     1      0             1    2001-01   \n",
       "2      2001-01-01-00     0            0     1      0             1    2001-01   \n",
       "3      2001-01-01-00     0            0     1      0             1    2001-01   \n",
       "4      2001-01-01-00     0            0     1      0             1    2001-01   \n",
       "5      2001-01-01-01     1            0     1      0             1    2001-01   \n",
       "6      2001-01-01-01     1            0     1      0             1    2001-01   \n",
       "7      2001-01-01-01     1            0     1      0             1    2001-01   \n",
       "8      2001-01-01-01     1            0     1      0             1    2001-01   \n",
       "9      2001-01-01-02     2            0     1      0             1    2001-01   \n",
       "\n",
       "   zone  passenger_count  \n",
       "0    43                1  \n",
       "1    48                1  \n",
       "2   132                2  \n",
       "3   161                2  \n",
       "4   239                2  \n",
       "5    97                1  \n",
       "6   143                1  \n",
       "7   230                2  \n",
       "8   237                1  \n",
       "9   231                2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_formatted</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>year_month</th>\n",
       "      <th>zone</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3344017</th>\n",
       "      <td>2024-04-01-20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344018</th>\n",
       "      <td>2024-04-01-22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344019</th>\n",
       "      <td>2024-04-01-22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344020</th>\n",
       "      <td>2024-04-02-09</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344021</th>\n",
       "      <td>2028-12-07-05</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2028-12</td>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344022</th>\n",
       "      <td>2028-12-07-05</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2028-12</td>\n",
       "      <td>170</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344023</th>\n",
       "      <td>2029-05-05-09</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2029-05</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344024</th>\n",
       "      <td>2029-05-05-12</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2029-05</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344025</th>\n",
       "      <td>2098-09-11-02</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2098-09</td>\n",
       "      <td>163</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344026</th>\n",
       "      <td>2098-09-11-03</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2098-09</td>\n",
       "      <td>231</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        datetime_formatted  hour  day_of_week  week  month  day_of_month  \\\n",
       "3344017      2024-04-01-20    20            0    14      3             1   \n",
       "3344018      2024-04-01-22    22            0    14      3             1   \n",
       "3344019      2024-04-01-22    22            0    14      3             1   \n",
       "3344020      2024-04-02-09     9            1    14      3             2   \n",
       "3344021      2028-12-07-05     5            3    49     11             7   \n",
       "3344022      2028-12-07-05     5            3    49     11             7   \n",
       "3344023      2029-05-05-09     9            5    18      4             5   \n",
       "3344024      2029-05-05-12    12            5    18      4             5   \n",
       "3344025      2098-09-11-02     2            3    37      8            11   \n",
       "3344026      2098-09-11-03     3            3    37      8            11   \n",
       "\n",
       "        year_month  zone  passenger_count  \n",
       "3344017    2024-04   264                1  \n",
       "3344018    2024-04   161                3  \n",
       "3344019    2024-04   249                1  \n",
       "3344020    2024-04   132                1  \n",
       "3344021    2028-12   132                3  \n",
       "3344022    2028-12   170                3  \n",
       "3344023    2029-05   231                1  \n",
       "3344024    2029-05   249                1  \n",
       "3344025    2098-09   163                5  \n",
       "3344026    2098-09   231                5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         datetime_formatted  hour  day_of_week  week  month  day_of_month  \\\n",
      "3344011 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344012 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344013 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344014 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344015 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344016 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344017 2024-04-01 20:00:00    20            0    14      3             1   \n",
      "3344018 2024-04-01 22:00:00    22            0    14      3             1   \n",
      "3344019 2024-04-01 22:00:00    22            0    14      3             1   \n",
      "3344020 2024-04-02 09:00:00     9            1    14      3             2   \n",
      "\n",
      "        year_month  zone  passenger_count  \n",
      "3344011    2024-04   125                1  \n",
      "3344012    2024-04   140                1  \n",
      "3344013    2024-04   143                2  \n",
      "3344014    2024-04   163                1  \n",
      "3344015    2024-04   236                1  \n",
      "3344016    2024-04   237                2  \n",
      "3344017    2024-04   264                1  \n",
      "3344018    2024-04   161                3  \n",
      "3344019    2024-04   249                1  \n",
      "3344020    2024-04   132                1  \n"
     ]
    }
   ],
   "source": [
    "combined_df['datetime_formatted'] = pd.to_datetime(combined_df['datetime_formatted'], format='%Y-%m-%d-%H')\n",
    "\n",
    "start_date = pd.Timestamp('2021-01-01')\n",
    "end_date = pd.Timestamp('2024-04-30 23:59:59')\n",
    "\n",
    "# Filter the DataFrame to include only the dates within the specified range\n",
    "combined_df = combined_df[(combined_df['datetime_formatted'] >= start_date) & (combined_df['datetime_formatted'] <= end_date)]\n",
    "\n",
    "# Check the last 10 rows of the filtered DataFrame\n",
    "print(combined_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to: c:\\Users\\35385\\Desktop\\CS_Summer_2024\\Shared_GH\\New-York-App\\data-analytics\\cleaning\\..\\Datasets\\taxi_other\\combined_df.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"Datasets\", \"taxi_other\")\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(data_dir, \"combined_df.csv\")\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "combined_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame saved to:\", file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp47350py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
